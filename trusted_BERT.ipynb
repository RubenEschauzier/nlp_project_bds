{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "trusted_BERT.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QllR7duZ8opl",
        "outputId": "4e165a29-2113-4166-a0ef-cb8a413d9084"
      },
      "source": [
        "!pip install transformers\n",
        "import sys\n",
        "import time\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import TensorDataset, random_split\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "import transformers\n",
        "from transformers import BertForSequenceClassification, AdamW, BertConfig, BertTokenizer, PreTrainedModel\n",
        "from transformers import get_linear_schedule_with_warmup, get_constant_schedule_with_warmup\n",
        "import datetime\n",
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/00/92/6153f4912b84ee1ab53ab45663d23e7cf3704161cb5ef18b0c07e207cef2/transformers-4.7.0-py3-none-any.whl (2.5MB)\n",
            "\u001b[K     |████████████████████████████████| 2.5MB 6.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from transformers) (3.13)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n",
            "\u001b[K     |████████████████████████████████| 901kB 48.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/e2/df3543e8ffdab68f5acc73f613de9c2b155ac47f162e725dcac87c521c11/tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3MB 35.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (4.5.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Collecting huggingface-hub==0.0.8\n",
            "  Downloading https://files.pythonhosted.org/packages/a1/88/7b1e45720ecf59c6c6737ff332f41c955963090a18e72acbcbeac6b25e86/huggingface_hub-0.0.8-py3-none-any.whl\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
            "Installing collected packages: sacremoses, tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.0.8 sacremoses-0.0.45 tokenizers-0.10.3 transformers-4.7.0\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "93tJtc0R83Cl"
      },
      "source": [
        "# New Section"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RIrpRmXa_Dw7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e123ebd1-cb13-40c0-c921-2f18abdf7d42"
      },
      "source": [
        "def process_text_data(data, tokenizer):\n",
        "    # I truncate the text if it's too large, might possibly cause issues not sure though\n",
        "    tokenized = data['Question'].apply((lambda x: tokenizer.encode(x, add_special_tokens=True, max_length=512, truncation=True)))\n",
        "    max_len = max([len(i) for i in tokenized.values])\n",
        "    padded = np.array([i + [0] * (max_len - len(i)) for i in tokenized.values])\n",
        "    attention_mask = np.where(padded != 0, 1, 0)\n",
        "\n",
        "    attention_masks = torch.tensor(attention_mask)\n",
        "    input_ids = torch.tensor(padded, dtype=torch.int64)\n",
        "\n",
        "    return input_ids, attention_masks\n",
        "\n",
        "\n",
        "def dataLoaders(dataset):\n",
        "    \"\"\"this function splits dataset, and creates dataloaders for training and validation sets.\"\"\"\n",
        "    train_size = int(0.9 * len(dataset))\n",
        "    val_size = len(dataset) - train_size\n",
        "    train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "    batch_size = 16\n",
        "\n",
        "    train_dataloader = DataLoader(\n",
        "        train_dataset,  # The training samples.\n",
        "        sampler=RandomSampler(train_dataset),  # Select batches randomly\n",
        "        batch_size=batch_size  # Trains with this batch size.\n",
        "    )\n",
        "\n",
        "    validation_dataloader = DataLoader(\n",
        "        val_dataset,  # The validation samples.\n",
        "        sampler=SequentialSampler(val_dataset),  # Pull out batches sequentially.\n",
        "        batch_size=batch_size  # Evaluate with this batch size.\n",
        "    )\n",
        "\n",
        "    return train_dataloader, validation_dataloader\n",
        "\n",
        "\n",
        "def load_model():\n",
        "    pass\n",
        "\n",
        "\n",
        "def format_time(elapsed):\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))  # Format as hh:mm:ss\n",
        "\n",
        "\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
        "\n",
        "def rmse(preds, actual):\n",
        "  preds = preds.squeeze()\n",
        "  return np.sqrt(np.sum((preds-actual)**2)/len(preds))\n",
        "\n",
        "\n",
        "def train_validate(model, scheduler, optimizer, epochs, train_dataloader, validation_dataloader, save_location = None):\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "\n",
        "    if device.type == 'cuda':\n",
        "        print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "\n",
        "    if device.type == 'cuda':\n",
        "        model.cuda()\n",
        "\n",
        "    training_stats = []\n",
        "    total_t0 = time.time()\n",
        "\n",
        "    for epoch_i in range(0, epochs):\n",
        "\n",
        "        print(\"\")\n",
        "        print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "        print('Training...')\n",
        "        t0 = time.time()\n",
        "        total_train_loss = 0\n",
        "        total_train_rmse = 0\n",
        "\n",
        "        # \"\"\"what happens here with drop out rate?\"\"\"\n",
        "        model.train()\n",
        "\n",
        "        for step, batch in enumerate(train_dataloader):\n",
        "            # \"\"\"each batch contains three pytorch tensors: input ids, attention masks, labels)\"\"\"\n",
        "            if step % 200 == 0 and not step == 0:\n",
        "                elapsed = format_time(time.time() - t0)\n",
        "                print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "            \n",
        "            b_input_ids = batch[0].to(device)\n",
        "            b_input_mask = batch[1].to(device)\n",
        "            b_labels = batch[2].to(device).float()\n",
        "\n",
        "            # one forward pass is performed on one epoch at the same time\n",
        "            # gradients are set to zero every time\n",
        "            # backward pass to capture gradients for back propagation\"\"\"\n",
        "            model.zero_grad()\n",
        "\n",
        "            outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
        "            loss = outputs.loss\n",
        "            logits = outputs.logits\n",
        "\n",
        "            # print(logits)\n",
        "            # print(b_labels)\n",
        "            # print(loss)\n",
        "\n",
        "\n",
        "            total_train_loss += loss.item()\n",
        "            loss.backward()\n",
        "\n",
        "            \"\"\" Clip the norm of the gradients to 1.0 to prevent the \"exploding gradients\" problem.\n",
        "            update parameters and learning rate\"\"\"\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "            logits = logits.detach().cpu().numpy()\n",
        "            label_ids = b_labels.cpu().numpy()\n",
        "            total_train_rmse += rmse(logits, label_ids)\n",
        "\n",
        "        \"\"\"calculate average loss over all examples\"\"\"\n",
        "        avg_train_loss = total_train_loss / len(train_dataloader)\n",
        "        avg_train_rmse = total_train_rmse / len(train_dataloader)\n",
        "        training_time = format_time(time.time() - t0)\n",
        "\n",
        "        print(\"\")\n",
        "        print(\"  Average training loss: {0:.4f}\".format(avg_train_loss))\n",
        "        print(\"  Average training rmse: {0:.4f}\".format(avg_train_rmse))\n",
        "\n",
        "        print(\"  Training epcoh took: {:}\".format(training_time))\n",
        "\n",
        "        \"\"\"measure our performance on our validation set\"\"\"\n",
        "        print(\"\")\n",
        "        print(\"Running Validation...\")\n",
        "        t0 = time.time()\n",
        "\n",
        "        \"\"\"evaluation mode makes sure that you can still get to the gradients even if drop out\"\"\"\n",
        "        model.eval()\n",
        "\n",
        "        total_eval_accuracy = 0\n",
        "        total_eval_loss = 0\n",
        "        nb_eval_steps = 0\n",
        "\n",
        "        for batch in validation_dataloader:\n",
        "            b_input_ids = batch[0].to(device)\n",
        "            b_input_mask = batch[1].to(device)\n",
        "            b_labels = batch[2].to(device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                \"\"\"forward pass, no grad as a graph is not necessary in forward prop\n",
        "                Get the \"logits\" output : values prior to activation function like the softmax.\"\"\"\n",
        "                outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
        "                loss = outputs.loss\n",
        "                logits = outputs.logits\n",
        "\n",
        "\n",
        "            total_eval_loss += loss.item()\n",
        "\n",
        "            \"\"\" Move logits and labels to CPU\"\"\"\n",
        "            logits = logits.detach().cpu().numpy()\n",
        "            label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "            \"\"\" calculate total accuracy over all batches.\"\"\"\n",
        "            total_eval_accuracy += rmse(logits, label_ids)\n",
        "\n",
        "        \"\"\" final accuracy for this validation run.\"\"\"\n",
        "        avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
        "        print(\"  RMSE: {0:.4f}\".format(avg_val_accuracy))\n",
        "\n",
        "        \"\"\" average loss over all of the batches.\"\"\"\n",
        "        avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
        "        validation_time = format_time(time.time() - t0)\n",
        "        print(\"  Validation Loss: {0:.4f}\".format(avg_val_loss))\n",
        "        print(\"  Validation took: {:}\".format(validation_time))\n",
        "\n",
        "        if save_location:\n",
        "          try:\n",
        "            ckpoint_name = 'chk_point{}'.format(epoch_i)\n",
        "            directory = os.path.join(save_location, ckpoint_name)\n",
        "            os.mkdir(directory)\n",
        "            model.save_pretrained(directory)\n",
        "            print('Saved model checkpoint to \\'{}\\''.format(directory))\n",
        "          except:\n",
        "            print('Something went wrong saving the model!')\n",
        "\n",
        "\n",
        "  \n",
        "\n",
        "    training_stats.append(\n",
        "        {\n",
        "            'epoch': epoch_i + 1,\n",
        "            'Training Loss': avg_train_loss,\n",
        "            'Valid. Loss': avg_val_loss,\n",
        "            'Valid. Accur.': avg_val_accuracy,\n",
        "            'Training Time': training_time,\n",
        "            'Validation Time': validation_time\n",
        "        }\n",
        "    )\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"Training complete!\")\n",
        "    print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time() - total_t0)))\n",
        "    if save_location:\n",
        "      try:\n",
        "        directory = os.path.join(save_location, 'final_model')\n",
        "        os.mkdir(directory)\n",
        "        model.save_pretrained(directory)\n",
        "        print('Saved model to \\'{}\\''.format(directory))\n",
        "      except:\n",
        "        print('Something went wrong saving the model!')\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    seed = 2021\n",
        "\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "    df = pd.read_csv(r'gdrive/MyDrive/nlp_bert/bert_input')\n",
        "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "\n",
        "    labels = torch.tensor(df['conflict_score'])\n",
        "    input_ids, attention_masks = process_text_data(df, tokenizer)\n",
        "\n",
        "    dataset = TensorDataset(input_ids, attention_masks, labels)\n",
        "\n",
        "    train_dl, val_dl = dataLoaders(dataset)\n",
        "\n",
        "    config = BertConfig.from_pretrained(\"bert-base-uncased\", num_labels=1)\n",
        "    pre_trained_model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", config=config)\n",
        "\n",
        "    # pre_trained_model = BertForSequenceClassification.from_pretrained(\n",
        "    #     \"bert-base-uncased\",  # Use the 12-layer BERT model, with an uncased vocab.\n",
        "    #     num_labels = 1 ,  # The number of output labels--2 for binary classification.\n",
        "    #     # You can increase this for multi-class tasks.\n",
        "    #     # I have to select True otherwise it will not compute cost etc) \n",
        "    # )\n",
        "\n",
        "    #pre_trained_model = transformers.BertForSequenceClassification.from_pretrained('gdrive/MyDrive/nlp_bert/bert_input)\n",
        "\n",
        "    optimizer_bert = AdamW(pre_trained_model.parameters(),\n",
        "                           lr=2e-5,  # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
        "                           eps=1e-10)  # args.adam_epsilon  - default is 1e-8.\n",
        "\n",
        "    num_epochs = 10  # Number of training epochs. Many epochs may be over-fitting training data.\n",
        "    total_steps = len(input_ids) * num_epochs  # total number of training steps\n",
        "    scheduler_bert = get_linear_schedule_with_warmup(optimizer_bert,\n",
        "                                                     num_warmup_steps=0,  # Default value in run_glue.py\n",
        "                                                     num_training_steps=total_steps)\n",
        "\n",
        "    train_validate(pre_trained_model, scheduler_bert, optimizer_bert, num_epochs, train_dl, val_dl, save_location = 'gdrive/MyDrive/nlp_bert/trained_model')"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla T4\n",
            "\n",
            "======== Epoch 1 / 10 ========\n",
            "Training...\n",
            "  Batch   200  of    528.    Elapsed: 0:00:38.\n",
            "  Batch   400  of    528.    Elapsed: 0:01:17.\n",
            "\n",
            "  Average training loss: 0.0232\n",
            "  Average training rmse: 0.1473\n",
            "  Training epcoh took: 0:01:41\n",
            "\n",
            "Running Validation...\n",
            "  RMSE: 0.1017\n",
            "  Validation Loss: 0.0113\n",
            "  Validation took: 0:00:03\n",
            "Something went wrong saving the model!\n",
            "\n",
            "======== Epoch 2 / 10 ========\n",
            "Training...\n",
            "  Batch   200  of    528.    Elapsed: 0:00:38.\n",
            "  Batch   400  of    528.    Elapsed: 0:01:16.\n",
            "\n",
            "  Average training loss: 0.0101\n",
            "  Average training rmse: 0.0971\n",
            "  Training epcoh took: 0:01:40\n",
            "\n",
            "Running Validation...\n",
            "  RMSE: 0.0895\n",
            "  Validation Loss: 0.0091\n",
            "  Validation took: 0:00:03\n",
            "Something went wrong saving the model!\n",
            "\n",
            "======== Epoch 3 / 10 ========\n",
            "Training...\n",
            "  Batch   200  of    528.    Elapsed: 0:00:38.\n",
            "  Batch   400  of    528.    Elapsed: 0:01:16.\n",
            "\n",
            "  Average training loss: 0.0068\n",
            "  Average training rmse: 0.0794\n",
            "  Training epcoh took: 0:01:40\n",
            "\n",
            "Running Validation...\n",
            "  RMSE: 0.0899\n",
            "  Validation Loss: 0.0088\n",
            "  Validation took: 0:00:03\n",
            "Something went wrong saving the model!\n",
            "\n",
            "======== Epoch 4 / 10 ========\n",
            "Training...\n",
            "  Batch   200  of    528.    Elapsed: 0:00:38.\n",
            "  Batch   400  of    528.    Elapsed: 0:01:15.\n",
            "\n",
            "  Average training loss: 0.0057\n",
            "  Average training rmse: 0.0724\n",
            "  Training epcoh took: 0:01:40\n",
            "\n",
            "Running Validation...\n",
            "  RMSE: 0.0876\n",
            "  Validation Loss: 0.0085\n",
            "  Validation took: 0:00:03\n",
            "Something went wrong saving the model!\n",
            "\n",
            "======== Epoch 5 / 10 ========\n",
            "Training...\n",
            "  Batch   200  of    528.    Elapsed: 0:00:38.\n",
            "  Batch   400  of    528.    Elapsed: 0:01:16.\n",
            "\n",
            "  Average training loss: 0.0050\n",
            "  Average training rmse: 0.0678\n",
            "  Training epcoh took: 0:01:40\n",
            "\n",
            "Running Validation...\n",
            "  RMSE: 0.0795\n",
            "  Validation Loss: 0.0072\n",
            "  Validation took: 0:00:03\n",
            "Something went wrong saving the model!\n",
            "\n",
            "======== Epoch 6 / 10 ========\n",
            "Training...\n",
            "  Batch   200  of    528.    Elapsed: 0:00:38.\n",
            "  Batch   400  of    528.    Elapsed: 0:01:16.\n",
            "\n",
            "  Average training loss: 0.0043\n",
            "  Average training rmse: 0.0627\n",
            "  Training epcoh took: 0:01:40\n",
            "\n",
            "Running Validation...\n",
            "  RMSE: 0.0807\n",
            "  Validation Loss: 0.0075\n",
            "  Validation took: 0:00:03\n",
            "Something went wrong saving the model!\n",
            "\n",
            "======== Epoch 7 / 10 ========\n",
            "Training...\n",
            "  Batch   200  of    528.    Elapsed: 0:00:38.\n",
            "  Batch   400  of    528.    Elapsed: 0:01:15.\n",
            "\n",
            "  Average training loss: 0.0035\n",
            "  Average training rmse: 0.0569\n",
            "  Training epcoh took: 0:01:40\n",
            "\n",
            "Running Validation...\n",
            "  RMSE: 0.0950\n",
            "  Validation Loss: 0.0099\n",
            "  Validation took: 0:00:03\n",
            "Something went wrong saving the model!\n",
            "\n",
            "======== Epoch 8 / 10 ========\n",
            "Training...\n",
            "  Batch   200  of    528.    Elapsed: 0:00:38.\n",
            "  Batch   400  of    528.    Elapsed: 0:01:16.\n",
            "\n",
            "  Average training loss: 0.0028\n",
            "  Average training rmse: 0.0513\n",
            "  Training epcoh took: 0:01:40\n",
            "\n",
            "Running Validation...\n",
            "  RMSE: 0.0891\n",
            "  Validation Loss: 0.0089\n",
            "  Validation took: 0:00:03\n",
            "Something went wrong saving the model!\n",
            "\n",
            "======== Epoch 9 / 10 ========\n",
            "Training...\n",
            "  Batch   200  of    528.    Elapsed: 0:00:38.\n",
            "  Batch   400  of    528.    Elapsed: 0:01:16.\n",
            "\n",
            "  Average training loss: 0.0023\n",
            "  Average training rmse: 0.0460\n",
            "  Training epcoh took: 0:01:40\n",
            "\n",
            "Running Validation...\n",
            "  RMSE: 0.0835\n",
            "  Validation Loss: 0.0079\n",
            "  Validation took: 0:00:03\n",
            "Something went wrong saving the model!\n",
            "\n",
            "======== Epoch 10 / 10 ========\n",
            "Training...\n",
            "  Batch   200  of    528.    Elapsed: 0:00:38.\n",
            "  Batch   400  of    528.    Elapsed: 0:01:16.\n",
            "\n",
            "  Average training loss: 0.0018\n",
            "  Average training rmse: 0.0414\n",
            "  Training epcoh took: 0:01:40\n",
            "\n",
            "Running Validation...\n",
            "  RMSE: 0.0844\n",
            "  Validation Loss: 0.0082\n",
            "  Validation took: 0:00:03\n",
            "Saved model checkpoint to 'gdrive/MyDrive/nlp_bert/trained_model/chk_point9'\n",
            "\n",
            "Training complete!\n",
            "Total training took 0:17:14 (h:mm:ss)\n",
            "Saved model to 'gdrive/MyDrive/nlp_bert/trained_model/final_model'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5auCPLiNvbYI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc409e47-d3e8-4e70-e05b-a8f61d164d47"
      },
      "source": [
        "import gensim\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "from tqdm import tqdm\n",
        "from gensim.models import Doc2Vec\n",
        "from gensim.models.doc2vec import TaggedDocument\n",
        "\n",
        "def baseline_prediction(x_train, x_test, y_train, y_test):\n",
        "    mean_score = np.mean(y_train)\n",
        "    mean_prediction = np.full(y_test.shape, mean_score)\n",
        "    baseline_performance = np.sqrt(np.sum((mean_prediction-y_test)**2)/mean_prediction.shape[0])\n",
        "    print('Baseline of predicting mean gives RMSE: {}'.format(baseline_performance))\n",
        "\n",
        "def create_document_vec(input_text):\n",
        "    documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(input_text)]\n",
        "    model = Doc2Vec(documents, vector_size=300, window=16, min_count=1)\n",
        "    return model\n",
        "\n",
        "\n",
        "df = pd.read_csv(r'gdrive/MyDrive/nlp_bert/bert_input')\n",
        "text_input = df['Question'].values\n",
        "labels_input = df['conflict_score'].values\n",
        "n_docs = text_input.shape[0]\n",
        "\n",
        "doc_vecs = create_document_vec(text_input)\n",
        "input_vectors = [doc_vecs[i] for i in range(n_docs)]\n",
        "X_train, X_test, y_train, y_test = train_test_split(input_vectors, labels_input, test_size=0.1, random_state=2021)\n",
        "baseline_prediction(X_train, X_test, y_train, y_test)\n",
        "\n",
        "reg = LinearRegression().fit(X_train, y_train)\n",
        "prediction = reg.predict(X_test)\n",
        "baseline_performance_linear = np.sqrt(np.sum((prediction-y_test)**2)/y_test.shape[0])\n",
        "print('Baseline of linear regression model: {}' .format(baseline_performance_linear))\n",
        "\n"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Baseline of predicting mean gives RMSE: 0.17506008251424307\n",
            "Baseline of linear regression model: 0.17643335638228275\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o2E9bD7rSTv8",
        "outputId": "dbf2b8e9-8a9c-4077-bf95-3e8bf0dca750"
      },
      "source": [
        "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "    config = BertConfig.from_pretrained(\"gdrive/MyDrive/nlp_bert/trained_model/chk_point5\", num_labels=1)\n",
        "    model = BertForSequenceClassification.from_pretrained(\"gdrive/MyDrive/nlp_bert/trained_model/chk_point5\", config=config)\n",
        "\n",
        "    questions = ['Why do you think people with vegan lifestyles are better?', 'Why do you think people with vegan lifestyles are annoying?', 'How would you cook brocolli?', 'Is milk vegan?']\n",
        "\n",
        "    tokenized = [tokenizer.encode(question, add_special_tokens=True, max_length=512, truncation=True) for question in questions]\n",
        "    max_len = max([len(i) for i in tokenized])\n",
        "    padded = np.array([i + [0] * (max_len - len(i)) for i in tokenized])\n",
        "    attention_mask = np.where(padded != 0, 1, 0)\n",
        "    padded = torch.IntTensor(padded)\n",
        "    attention_mask = torch.IntTensor(attention_mask)\n",
        "    outputs = model.forward(padded, attention_mask=attention_mask)\n",
        "    print(outputs)\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SequenceClassifierOutput(loss=None, logits=tensor([[ 0.3627],\n",
            "        [ 0.3327],\n",
            "        [-0.0110],\n",
            "        [-0.0085]], grad_fn=<AddmmBackward>), hidden_states=None, attentions=None)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}